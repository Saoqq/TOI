# 4 - ГСВ с одинаковыми матрицами ковариации

## ![scheme](ГСВ%20с%20одинаковыми%20матрицами%20ковариации.svg)

1) Входные данные:
   + кол-во классов M;
   + априорные вероятности классов $p(\omega_i), i = \overline {1, M}$;
   + функции правдоподобия  $f_x (x| \omega_i)= p (x|\omega_i) = N(x, m_i, c_i), i = \overline {1, M}, \;$
   где $N(x,m,c) = \frac{1}{(2 \pi)^{\frac{n}{2}} |С|^{\frac{1}{2}}} e^{-\frac{1}{2} (x - m)^T С^{-1} (x-m)}$;
   + матрицы ковариации классов $C_i$, которые в данном случае у всех классов одинаковы;
   + математические ожидания классов $m_i$;
   + образ $x$.

2) Будем использовать в качестве критерия максимума апостериорной вероятности: $g_i(x)=p(\omega_0) * p(x|\omega_i)$.

   Для удобства применим преобразование $g'_i(x)=ln(g_i(x))$ и вычтем общую константу $-(\frac{n}{2})ln(2\pi)$: 
   $$ 
   g'_i(x) = -\frac{1}{2} ln|C| - \frac{1}{2}(x-m_i)^T C^{-1} (x-m_i) + ln(p(\omega_i)), \; i=\overline{1,M} ;
   $$
   $$ 
   g'_i(x) = -\frac{1}{2} ln|С| - \frac{1}{2}x^T C^{-1} x + x^T C^{-1}  
   - \frac{1}{2}m_i^T C^{-1} m + ln(p(\omega_i)), \; i=\overline{1,M};
   $$
   т.к. первые два слагаемых всегда меньше 0 и являются общими для всех разделяющих функций, то их можно вычесть и получить решающее правило вида:
   $$ 
   g''_i(x) = x^T C^{-1} m_i - \frac{1}{2} m_i^T C^{-1} m_i + ln(p(\omega_i)), \; i=\overline{1,M} 
   $$

  
3) Решение принимается в пользу того класса, которому соответствует максимальное значение разделяющей функции для данного класса  
   $$
   \omega_i : g''_i(x) \geq g''_j(x), \;\; i=\overline{1,M}, \;\; j=\overline{1,M}, \;\; i \neq j
   $$ 
   или можно переписать в следующем виде:
   $$
   g'''_{ij}(x) =  x^T C^{-1}(m_i-m_j)- \frac{1}{2} (m_i + m_j) C^{-1} (m_i-m_j),
   $$
   $$
   l'_{0ij} = ln(\frac{p(\omega_j)}{p(\omega_i)}),
   $$  
   $$
   \omega_i:
   g'''_{ij}
   \def\arraystretch{0.6}
   \begin{array}{c}
   \small \omega_i\\
   {>}\\
   <\\
   \small \omega_j
   \end{array}
   l'_{0ij}.
   $$

   Получившиеся разделяющиеся функции являются линейными функциями признаков. Это означает, что границы областей решений имеют линейный характер. В случае более двух классов они могут быть получены каак фрагменты гиперплоскости, производящие попарное разделение классов:

   $$
   g''_{ij}(x) =  x^T C^{-1}(m_i-m_j) - \frac{1}{2} (m_i + m_j) C^{-1} (m_i-m_j) - l'_{0ij}
   $$
   
   - - - 
   ### Пример для случая двух классов

    $$
   g'''_{ij}(x) =  x^T C^{-1}(m_1-m_2)- \frac{1}{2} (m_1 + m_2) C^{-1} (m_1-m_2) - l'_{0ij};
   $$
   $$
   l'_{0} = ln(\frac{p(\omega_2)}{p(\omega_1)});
   $$  
   $$
   \omega_i:
   g'''_{ij}
   \def\arraystretch{0.6}
   \begin{array}{c}
   \small \omega_1\\
   {>}\\
   <\\
   \small \omega_2
   \end{array}
   l'_{0}.
   $$

   Для того, чтобы оценить правильность работы алгоритам, нужно рассчитать вероятности ошибок. В данном случае, т.к. в качестве разделяющей функции $g'''(x) взят логарифм отношения правдоподобия, то вероятности ошибок можно рассчитать по следующим формулам:

   $$
   \alpha= \int\limits_{-\infty}^{l'_0} p(g'''|\omega_1 )dg''',\;
   \beta=  \int\limits_{l'_0}^{\infty} p(g'''|\omega_2 )dg''';
   $$

   Поскольку $g'''(x)$ является линейной комбинацией ГСВ, то оно тоже является ГСВ, поэтому:
   $$
   p (g'''|\omega_1) = N(g''', m_g1, D_p1), \; 
   p (g'''|\omega_2) = N(g';', m_g2, D_p2)
   $$




расчёты на стр. 3-4 из [лекции 4](../лекции%20с%20мудла/v4.pdf)