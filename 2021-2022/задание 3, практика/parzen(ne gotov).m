%Реализовать алгоритм распознавания ГСВ методом имитационного моделирования на основе оценки плотности вероятности методом Парзена (функция vkernel(x,XN,h_N,kl_kernel): ядро -- гауссовская функция c использованием диагональной матрицы, r=0.5, h_N считать на основе обучающей выборки) при следующих исходных данных.
% pw1=0.3; pw2=0.7;
% N1=6000; N2=14000;
% (объем тестовых данных каждого класса)
% (объем обучающих данных должен составлять 1/10
% от этих чисел для каждого класса соответственно)
% m1=[1.5, 2.0, 1.0]'; 
% m2=[1.0,3.0,1.0]';
% C1=[1.5, 0.1, 0.0;0.1, 2.0, 0.2;0.0, 0.2, 2.0;];
% C2=[2.0, -0.2, 0.0;-0.2, 2.0, 0.2;0.0, 0.2, 2.0;];
% При генерации выборок классов использовать начальную установку генератора СВ rng(15) (в начале кода один раз) и функцию randncor(n,N,C).
% Сначала генерировать обучающие данные для одного класса, потом -- второго.
% Затем -- тестовые данные для одного класса и второго.
% Подсчитать суммарную ошибку распознавания (отношение количества ошибок перепутывания обоих классов к общему числу экспериментов N1+N2) до четвертого знака после запятой.
%
%Изначально - Файл pr53_rec_gaus_uneq. Синтез и анализ алгоритмов распознавания ГСВ с 
%различными матрицами ковариации
clear all; close all;
% Построить  график  зависимости  суммарной  экспериментальной  ошибки 
% первого рода ( для  третьего  класса)  от числа испытаний  (объема выборки ). 
% Сравнить с теоретическим  значением. 

% Нужно добавить вот эти строки
% KK = [100 500 1000 2000 5000 10000];    % разные значения объемов выборки
rng(15);

pw1=0.3; pw2=0.7;

N1=6000; N2=14000;
N1_ob = 600;
N2_ob = 1400;


 m1=[1.5, 2.0, 1.0]; 

 m2=[1.0,3.0,1.0];
 
 C1=[1.5, 0.1, 0.0;

        0.1, 2.0, 0.2;

        0.0, 0.2, 2.0;];

 C2=[2.0, -0.2, 0.0;

        -0.2, 2.0, 0.2;

        0.0, 0.2, 2.0;];
    
KK = 20000;
err1c3 = zeros(size(KK));                % ошибка первого рода третьего класса
err2c3 = zeros(size(KK));                % ошибка второго рода третьего класса

% Добавляется цикл по объемам выборки
% for tt = 1 : numel(KK)  % цикл по объемам выборки
    %1.Задание исходных данных
    n=3;M=2;%%размерность признакового пространства и число классов
    
    % МЕНЯЕТСЯ СЛЕДУЮЩАЯ СТРОЧКА:
    K = KK; % K=1000;%количество статистических испытаний
    
    %Априорные вероятности, математические ожидания и матрицы ковариации классов
    dm=2.0;%расстояние между математическими ожиданиями классов по координатным осям
    C=zeros(n,n,M); C_=C;%матрица ковариации вектора признаков различных классов
    pw=[pw1 pw2];
    pw=pw/sum(pw);
    D=3*eye(2);
    m=[m1; m2]';
    
    C(:,:,1)=[1.5, 0.1, 0.0;

        0.1, 2.0, 0.2;

        0.0, 0.2, 2.0;];
    C(:,:,2)=[2.0, -0.2, 0.0;

        -0.2, 2.0, 0.2;

        0.0, 0.2, 2.0;];
    for k=1:M,
        C_(:,:,k)=C(:,:,k)^-1; 
    end;
    np=sum(pw); pw=pw/np; %исключение некорректного задания априорных вероятностей
    
    % + Этот пункт 1.1. Генерация обучающих выборок классов
    % число образов каждого класса
%     Ks = fix(K * pw);
%     Ks(end) = K - sum(Ks(1 : end - 1));
    Ks = N1_ob;
    XN{1} = repmat(m(:,1), [1, Ks]) + randncor(n,Ks,C(:,:,1)); %генерация К образов 1-го класса
    	
    Ks = N2_ob;
    XN{2} = repmat(m(:,2), [1, Ks]) + randncor(n,Ks,C(:,:,2)); %генерация К образов 2-го класса
    
    
    %2.Расчет матриц вероятностей ошибок распознавания
    PIJ=zeros(M); PIJB=zeros(M); mg=zeros(M); Dg=zeros(M); l0_=zeros(M);    
    for i=1:M,
        for j=i+1:M,
               dmij=m(:,i)-m(:,j); 
               l0_(i,j)=log(pw(j)/pw(i)); 
               dti=det(C(:,:,i)); dtj=det(C(:,:,j));
               trij=trace(C_(:,:,j)*C(:,:,i)-eye(n)); trji=trace(eye(n)-C_(:,:,i)*C(:,:,j));
               mg1=0.5*(trij+dmij'*C_(:,:,j)*dmij-log(dti/dtj)); 
               Dg1=0.5*trij^2+dmij'*C_(:,:,j)*C(:,:,i)*C_(:,:,j)*dmij; 
               mg2=0.5*(trji-dmij'*C_(:,:,i)*dmij+log(dtj/dti)); 
               Dg2=0.5*trji^2+dmij'*C_(:,:,i)*C(:,:,j)*C_(:,:,i)*dmij; 
               sD1=sqrt(Dg1); sD2=sqrt(Dg2);
               PIJ(i,j)=normcdf(l0_(i,j),mg1,sD1); 
               PIJ(j,i)=1-normcdf(l0_(i,j),mg2,sD2);
               mu2=(1/8)*dmij'*((C(:,:,i)/2+C(:,:,j)/2)^-1)*dmij...
                   +0.5*log((dti+dtj)/(2*sqrt(dti*dtj)));%расстояние Бхатачария
               PIJB(i,j)=sqrt(pw(j)/pw(i))*exp(-mu2);PIJB(j,i)=sqrt(pw(i)/pw(j))*exp(-mu2);%границы Чернова
        end;
        PIJB(i,i)=1-sum(PIJB(i,:));
        PIJ(i,i)=1-sum(PIJ(i,:));%нижняя граница вероятности правильного распознавания
     end;
     
    %+2.1.Определение вероятностей ошибок методом скользящего контроля
    % Убрать отсюда всё, где есть Pc2 и p2_
	r=0.5; kl_kernel=11;	% параметры оценки Парзена
    Pc1=zeros(M);%матрицы ошибок
    p1_=zeros(M,1);
    Ks = [600 1400];
    for i=1:M,%реализация метода скользящего контроля
        N=Ks(i);
        XNi=XN{i}; XNi_=zeros(n,N-1);
        indi=[1:i-1,i+1:M];
        for j=1:N,
            x=XNi(:,j); indj=[1:j-1,j+1:N];%изъятие тестового образа i-го класса
            XNi_(:,1:j-1)=XNi(:,1:j-1); 
            XNi_(:,j:end)=XNi(:,j+1:end);
            h_N=N^(-r/n);       % + %размеры окна Парзена
            p1_(i)=vkernel(x,XNi_,h_N,11);%оценка Парзена
            for t=1:M-1,
                 ij=indi(t);
                 h_N=Ks(ij)^(-r/n);       % + %размеры окна Парзена
                 p1_(ij)=vkernel(x,XN{ij},h_N,11);
            end;
            [ui1,iai1]=max(p1_);
            Pc1(i,iai1)=Pc1(i,iai1)+1;
        end;
        Pc1(i,:)=Pc1(i,:)/N;
    end;
     
    %3.Тестирование алгоритма методом статистических испытаний
    Pcv=zeros(M); p=zeros(M,1); % +
    
    x=ones(n,1); u=zeros(M,1);
    Pc_=zeros(M);%экспериментальная матрица вероятностей ошибок
    for k=1:K,%цикл по числу испытаний
        for i=1:M,%цикл по классам
            [x,px]=randncor(n,1,C(:,:,i)); x=x+m(:,i);%генерация образа i-го класса             
            for j=1:M,%вычисление значения разделяющих функций
                u(j)=-0.5*(x-m(:,j))'*C_(:,:,j)*(x-m(:,j))-0.5*log(det(C(:,:,j)))+log(pw(j));                
                h_N=Ks(j)^(-r/n);       % + %размеры окна Парзена
                p(j)=vkernel(x, XN{j}, h_N, 11);    % +
            end;
            [ui,iai]=max(u);%определение максимума
            Pc_(i,iai)=Pc_(i,iai)+1;%фиксация результата распознавания
            
            [ui,iai]=max(p);        % + %определение максимума
            Pcv(i,iai)=Pcv(i,iai)+1;% + %фиксация результата распознавания
        end;
    end;
    Pc_=Pc_/K;
    Pcv=Pcv/K;  % +

    % В конце шага цикла фиксируем очередное значение ошибки
    err1c3(tt) = Pc_(3, 1) + Pc_(3, 2); % третий класс первый род (сумма по строке)
    err2c3(tt) = Pc_(1, 3) + Pc_(2, 3); % третий класс второй род (сумма по столбцу)
	
	err1c3(tt) = Pcv(3, 1) + Pcv(3, 2); % третий класс первый род (сумма по строке)
    err2c3(tt) = Pcv(1, 3) + Pcv(2, 3); % третий класс второй род (сумма по столбцу)
% end % конец цикла по объемам выборки

% ТЕПЕРЬ визуализация зависимостей ошибок от объема выборки
 figure; hold on;    % новое графическое окно + режим дорисовки
 plot(KK, err1c3);   % график экспериментальной ошибки
 title('первый класс первый род');

 figure; hold on;    % новое графическое окно + режим дорисовки
 plot(KK, err2c3);   % график экспериментальной ошибки
 title('второй класс первый род');

disp('Теоретическая матрица вероятностей ошибок');disp(PIJ);
disp('Матрица ошибок по методу скользящего контроля');disp(Pc1);
disp('Матрица ошибок на основе границы Чернова');disp(PIJB);
disp('Экспериментальная матрица ошибок (гауссовский классификатор)');disp(Pc_);
disp('Экспериментальная матрица ошибок (с оценками Парзена)');disp(Pcv);







