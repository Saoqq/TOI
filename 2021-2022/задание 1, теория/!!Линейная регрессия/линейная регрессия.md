# Линейная регрессия

## ![scheme](линейная%20регрессия.svg)


Далее не этот алгорит, оставил как пример
____________________________________________________________
1) Входными данными для этого алгоритма являются:

   + образ $x$
   + кол-во классов M
   + априорные вероятности классов $p(\omega_i), i = \overline {1, M}$
   + функции правдоподобия  $f_x (x| \omega_i)= p (x|\omega_i) = N(x, m_i, C_i), i = \overline {1, M}$  
   Где $N(x,m,c) = \frac{1}{(2 \pi)^{\frac{n}{2}} |c|^{\frac{1}{2}}} e^{-\frac{1}{2} (x - m)^T C^{-1} (x-m)}$  
   + матрицы ковариации $C_i$ и математические ожидания $m_i$ для классов

2) Будем использовать в качестве критерия MAB <em>(максимум апостериорной вероятности)</em> :
   $$g_i(x)=p(\omega_i) * p(x|\omega_i)$$ 
   Для удобства применим преобразование $g'_i(x)=ln(g_(x))
   $
   и вычтем общую константу $-(\frac{n}{2})ln(2\pi)$  
   $$g'_i(x) = -\frac{1}{2} ln(C_i) - \frac{1}{2}(x-m_i)^T C^{-1}_i (x-m_i) + ln(p(\omega_i)), i=\overline{1,M}$$
3) Решение принимаются в пользу того класса, которому соответств макс значение разделяющей функции для данного класса  
   $\omega_i : g'_i(x) \geq g'_j(x), i=\overline{1,M}, i \neq j$  
или можно переписать в следующем виде  
   $g''_{ij}(x) = -\frac{1}{2}ln(\frac{|C_i|}{C_j}) - \frac{1}{2} x^T (C^{-1}_1 - C^{-1}_2)x + x^T(C^{-1}_1 m_1 - C^{-1}_2 m_2) - \frac{1}{2} m_1^T C_1^{-1}m_1 + \frac{1}{2} m_2^T C_2^{-1}$  
   $l'_{0ij} = ln(\frac{p(\omega_j)}{p(\omega_i)})$  
   $
   \omega_i:
   g''_{ij}
   \def\arraystretch{0.6}
   \begin{array}{c}
   \small w_i\\
   {>}\\
   <\\
   \small w_j
   \end{array}
   l'_{0ij}
   $  
   В данном случае зависимость между разделяющей функцией и вектором признаков является квадратичной, следовательно, граница областей решений будет иметь норму разделичных поверхностей второго порядка.  
  
Пример для случая двух классов и вычисляются ошибки в [лекции 4](../лекции%20с%20мудла/v4.pdf) на стр 10-15.  
<font color="red"><em>Прим. У прошлого года была страница 7-10, файл лекции изменился<em></font>
